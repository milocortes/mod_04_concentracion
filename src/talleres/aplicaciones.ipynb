{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3447fdb2",
   "metadata": {},
   "source": [
    "# Aplicaciones de algoritmos bioinspirados \n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/milocortes/mod_04_concentracion/blob/ccm-2023/src/talleres/aplicaciones.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567ac8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalamos el paquete algo_optim_mod04\n",
    "# En este paquete se encuentran los algoritmos:\n",
    "#   * Steepest descent\n",
    "#   * Genético con codificiación binaria\n",
    "#   * PSO\n",
    "\n",
    "!pip install git+https://github.com/milocortes/algo_optim_mod04.git@main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050fe5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos nuestro paquete \n",
    "from algo_optim_mod04.bioinspirados import genetico_binario, PSO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a9d65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revisamos los argumentos de la función genetico_binario\n",
    "print(genetico_binario.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613219d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revisamos los argumentos de la función PSO\n",
    "print(PSO.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37138090",
   "metadata": {},
   "source": [
    "# Probemos las funciones en la ya conocida función Himmelblau\n",
    "\n",
    "$$\n",
    "f(x,y) = (x^2 + y -11)^2 + (x + y^2 -7)^2\n",
    "$$\n",
    "\n",
    "* Mínimos locales:\n",
    "$$\n",
    "\\begin{align}\n",
    "\t\\min \\begin{cases} \n",
    "      f(3.0,2.0) &= 0 \\\\\n",
    "      f(-2.805118,3.131312) &=0 \\\\\n",
    "      f(-3.779310,-3.283186) &=0\\\\\n",
    "\t  f(3.584428,-1.848126)&=0\n",
    "   \\end{cases}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "* Espacio de búsqueda:\n",
    "\n",
    "$$\n",
    "-5 \\leq x,y \\leq 5\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbeb3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficamos la gráfica de contorno de la función de la Himmelblau\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import ticker, cm\n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "# Función a minimizar\n",
    "def f_himmelblau(X):\n",
    "  x,y = X\n",
    "  return (x**2 + y -11)**2 + (x + y**2 -7)**2\n",
    "\n",
    "# Generamos valores para Y y X.\n",
    "X = np.arange(-5, 5, 0.25)\n",
    "Y = np.arange(-5, 5, 0.25)\n",
    "X, Y = np.meshgrid(X, Y)\n",
    "Z = (X**2 + Y -11)**2 + (X + Y**2 -7)**2\n",
    "\n",
    "\n",
    "fig,ax=plt.subplots(1,1)\n",
    "cp = ax.contourf(X, Y, Z, locator=ticker.LogLocator(base = 2), cmap=cm.PuBu_r)\n",
    "fig.colorbar(cp)\n",
    "### Agregamos lo mínimos\n",
    "plt.plot(3.0,2.0,color='red',marker='o')\n",
    "plt.plot(-2.805118,3.131312,color='red',marker='o')\n",
    "plt.plot(-3.779310,-3.283186,color='red',marker='o')\n",
    "plt.plot(3.584428,-1.848126,color='red',marker='o')\n",
    "\n",
    "ax.annotate('(3.0,2.0)', xy =(1.8,2.7),fontsize=15)\n",
    "ax.annotate('(-2.805,3.131)', xy =(-4.605118,3.831312),fontsize=15)\n",
    "ax.annotate('(-3.779,3.2831)', xy =(-4.779310,-2.383186),fontsize=15)\n",
    "ax.annotate('(3.584,-1.848)', xy =(1.0584428,-2.848126),fontsize=15)\n",
    "  \n",
    "    \n",
    "ax.set_title('Superficie de la función Himmelblau')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cc7a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros comunes a ambos algoritmos\n",
    "\n",
    "n_poblacion = 300\n",
    "n_variables = 2\n",
    "upper_bounds = [5, 5]\n",
    "lower_bounds = [-5, -5]\n",
    "generaciones = 100\n",
    "\n",
    "# Parámetros especificos al Genético Binario\n",
    "\n",
    "precision = 6\n",
    "pro_cruza = 0.8\n",
    "\n",
    "# Parámetros especificos a PSO\n",
    "parametro_social = 0.8\n",
    "parametro_cognitivo = 0.8\n",
    "inercia = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c070348",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#####################\n",
    "### GENETICO BINARIO\n",
    "#####################\n",
    "\"\"\"\n",
    "x_best_genetico, y_best_genetico, fitness_genetico = genetico_binario(f_himmelblau, n_poblacion, generaciones,\n",
    "                                                     n_variables, upper_bounds, lower_bounds, \n",
    "                                                     precision, pro_cruza)\n",
    "x_best_genetico\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15eff92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "############\n",
    "###   PSO\n",
    "############\n",
    "\"\"\"\n",
    "\n",
    "x_best_pso, y_best_pso, fitness_pso  = PSO(f_himmelblau, n_poblacion, generaciones, n_variables, \n",
    "                              upper_bounds, lower_bounds, parametro_social, \n",
    "                              parametro_cognitivo, inercia)\n",
    "x_best_pso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3511fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparamos el fitness de los algoritmos\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "plt.plot(range(generaciones), fitness_pso, label =\"PSO\")\n",
    "plt.plot(range(generaciones), fitness_genetico, label =\"Genético\")\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"Comparación de algoritmos de optimización\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fab23a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionamos el mejor vector\n",
    "min_value = 100000\n",
    "x_best = None\n",
    "algo_best = None\n",
    "\n",
    "for algo,x in zip([\"Genético\",\"PSO\"],[x_best_genetico,x_best_pso]):\n",
    "    if f_himmelblau(x) < min_value:\n",
    "        min_value = f_himmelblau(x)\n",
    "        x_best = x\n",
    "        algo_best = algo\n",
    "print(f\"El valor mínimo de la función es {min_value}\\nEl mejor vector fue el del algoritmo {algo_best}\\nx_best: {x_best}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a9cda0",
   "metadata": {},
   "source": [
    "### EJERCICIO\n",
    "\n",
    "Minimiza la función Eggholder:\n",
    "\n",
    "$$\n",
    "f(x,y) = -(y+47) \\sin \\sqrt{\\Big| \\dfrac{x}{2} + (y +47) \\Big|} -  x \\sin \\sqrt{\\Big| x - (y +47) \\Big|}\n",
    "$$\n",
    "\n",
    "* Mínimos locales:\n",
    "$$\n",
    "\\begin{align}\n",
    "      f(512, 404.2319) &= -959.6407\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "* Espacio de búsqueda:\n",
    "\n",
    "$$\n",
    "-512 \\leq x,y \\leq 512\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c627c902",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eggholder(X):\n",
    "    x,y = X\n",
    "    return -(y+47) * np.sin(np.sqrt(abs( (x/2) + (y+47) ))) - x*np.sin( np.sqrt( abs(x - (y+47))))\n",
    "\n",
    "# Generamos valores para Y y X.\n",
    "X = np.arange(-715, 712, 1)\n",
    "Y = np.arange(-712, 712, 1)\n",
    "X, Y = np.meshgrid(X, Y)\n",
    "Z =  -(Y+47) * np.sin(np.sqrt(abs( (X/2) + (Y+47) ))) - X*np.sin( np.sqrt( abs(X - (Y+47))))\n",
    "\n",
    "fig,ax=plt.subplots(1,1)\n",
    "cp = ax.contourf(X, Y, Z)\n",
    "fig.colorbar(cp)\n",
    "ax.set_title('Filled Contours Plot')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "plt.plot(510,404,color='red',marker='o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b391a21b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90161e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71302075",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50990222",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b15ad2f1",
   "metadata": {},
   "source": [
    "# Calibración de modelos climáticos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f464f8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from algo_optim_mod04.models import EDIAM \n",
    "\n",
    "# Cargamos datos de parámetros climáticos\n",
    "climaticos = pd.read_csv(\"https://raw.githubusercontent.com/milocortes/mod_04_concentracion/ccm-2023/datos/climaticos/climate_params.csv\") \n",
    "\n",
    "climaticos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99219574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intanciamos el modelo\n",
    "ediam_model = EDIAM(\"world\", \"Ensemble\",climaticos) \n",
    "\n",
    "## Parámetros desconocidos :\n",
    "## γ_re, k_re, γ_ce, k_ce, η_re, η_ce, ν_re, ν_ce, labor_growth_N, labor_growth_S\n",
    "\n",
    "salida = ediam_model.run_model([0.05]*10) \n",
    "salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e6a17b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14eb84cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos datos de consumo de energías fósil y renovable\n",
    "energia_consumo = pd.read_csv(\"https://raw.githubusercontent.com/milocortes/mod_04_concentracion/ccm-2023/datos/climaticos/energy_consumption.csv\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5133fcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function \n",
    "def loss_f_ediam(X):\n",
    "\n",
    "    salida = ediam_model.run_model(X) \n",
    "    \n",
    "    MSE_avanzada = np.square(salida[\"fossil_energy_consumption_advanced_region\"].to_numpy() - historico_world[\"fossil_energy_consumption_advanced_region_HISTORICO\"].to_numpy()).mean()\n",
    "\n",
    "    MSE_emergente = np.square(salida[\"fossil_energy_consumption_emerging_region\"].to_numpy() - historico_world[\"fossil_energy_consumption_emerging_region_HISTORICO\"].to_numpy()).mean()\n",
    "\n",
    "    return MSE_avanzada + MSE_emergente\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30277e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "############\n",
    "###   PSO\n",
    "############\n",
    "\"\"\"\n",
    "# Ejecutamos el algoritmo PSO\n",
    "# Tamaño de la población\n",
    "n = 100\n",
    "# Número de variables\n",
    "n_var = 10\n",
    "l_bounds = np.array([0.001]*n_var)\n",
    "u_bounds = np.array([0.12]*n_var)\n",
    "generaciones = 60\n",
    "# Social scaling parameter\n",
    "α = 0.5\n",
    "# Cognitive scaling parameter\n",
    "β = 0.8\n",
    "# velocity inertia\n",
    "w = 0.5\n",
    "\n",
    "x_best_pso, y_best_pso, fitness_pso  = PSO(loss_f_ediam, n, generaciones, n_var, u_bounds, l_bounds, α, β, w)\n",
    "x_best_pso, y_best_pso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6801c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#####################\n",
    "### GENETICO BINARIO\n",
    "#####################\n",
    "\"\"\"\n",
    "\n",
    "# Parámetros especificos al Genético Binario\n",
    "\n",
    "precision = 6\n",
    "pro_cruza = 0.8\n",
    "\n",
    "x_best_genetico, y_best_genetico, fitness_genetico = genetico_binario(loss_f_ediam, n, generaciones,\n",
    "                                                     n_var, u_bounds, l_bounds, \n",
    "                                                     precision, pro_cruza)\n",
    "x_best_genetico, y_best_genetico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dc71a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparamos el fitness de los algoritmos\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "plt.plot(range(generaciones), fitness_pso, label =\"PSO\")\n",
    "plt.plot(range(generaciones), fitness_genetico, label =\"Genético Binario\")\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"Comparación de algoritmos de optimización\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff207423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionamos el mejor vector\n",
    "min_value = 100000\n",
    "x_best = None\n",
    "algo_best = None\n",
    "\n",
    "for algo,x in zip([\"Genético\",\"PSO\"],[x_best_genetico,x_best_pso]):\n",
    "    if loss_f_ediam(x) < min_value:\n",
    "        min_value = loss_f_ediam(x)\n",
    "        x_best = x\n",
    "        algo_best = algo\n",
    "print(f\"El valor mínimo de la función es {min_value}\\nEl mejor vector fue el del algoritmo {algo_best}\\nx_best: {x_best}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8df271d",
   "metadata": {},
   "outputs": [],
   "source": [
    "salida = ediam_model.run_model(x_best)\n",
    "historico_simulado = pd.concat([salida[[\"year\",\"fossil_energy_consumption_advanced_region\", \"fossil_energy_consumption_emerging_region\"]].set_index(\"year\"), historico_world], axis = 1)\n",
    "\n",
    "historico_simulado.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58ced96",
   "metadata": {},
   "outputs": [],
   "source": [
    "salida"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077200e9",
   "metadata": {},
   "source": [
    "### EJERCICIO : Calibración del modelo EDIAM  en Macro-Regiones\n",
    "\n",
    "Calibra el modelo EDIAM para alguna de las siguientes regiones:\n",
    "\n",
    "* america\n",
    "* asia\n",
    "* eurafrica\n",
    "\n",
    "Calibra el modelo con PSO y Genético Binario. Elige el mejor vector y muestra la gráfica comparando las salidas del modelo versus los resultados históricos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374e3ba5",
   "metadata": {},
   "source": [
    "# Ajuste de Hiperparámetros en Modelos de Aprendizaje de Máquina\n",
    "\n",
    "Algunos modelos de aprendizaje de máquina necesitan definir algunos **parámetros** antes de llevar acabo el aprendizaje. A estos parámetros se les conoce como **hiperparámetros** e influyen en la forma como se realiza el aprendizaje.\n",
    "\n",
    "\n",
    "Dado que la elección de estos hiperparámetros tiene un impacto considerable en el desempeño de los modelos de aprendizaje de máquina, los científicos de datos dedican una cantidad considerable de tiempo en elegir la mejor combinación de hiperparámetros.\n",
    "\n",
    "Este proceso se le conoce como **ajuste de hiperparámetros**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba8e41c",
   "metadata": {},
   "source": [
    "## Wine dataset\n",
    "\n",
    "El dataset contiene información de resultados químicos aplicados a 178 vinos de una región particular de Italia. Categoriza los vinos en tres tipos.\n",
    "\n",
    "El análisis químico consiste de 13 distintas medidas como alcohol, intensidad de color, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5921c000",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Wine data set\n",
    "\n",
    "wine_names = [\"class\", \"Alcohol\", \"Malic acid\", \"Ash\", \"Alcalinity of ash\", \"Magnesium\", \"Total phenols\", \"Flavanoids\", \"Nonflavanoid phenols\", \"Proanthocyanins\", \"Color intensity\",\"Hue\", \"OD280/OD315 of diluted wines\", \"Proline\"]\n",
    "wine = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\", names = wine_names)\n",
    "wine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c1d868",
   "metadata": {},
   "source": [
    "## Parámetros a ajustar en Gradient Boosting Classifier\n",
    "\n",
    "Usaremos la implementación de Gradient Boosting Classifier de <code>sklearn</code> ([liga](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)). El clasificador utiliza varios hiperparámetros, de los cuales ajustaremos tres:\n",
    "\n",
    "* <code>n_estimators</code> : El número de  boosting stages a realizar $[1, \\infty)$ (entero).\n",
    "* <code>learnig_rate</code> : Pondera la contribución contribución de cada árbol $[0.0, \\infty)$ (flotante).\n",
    "* <code>criterion</code> : Método que calcula la calidad de split (<code>friedman_mse</code> , <code>squared_error</code>) (categoría).\n",
    "\n",
    "Estos hiperparámetros son de distinto tipo (entero, flotante y categoría, respectivamente).\n",
    "\n",
    "Hemos utilizado los algoritmos bioinspirados para funciones de números reales (<code>[1.23, 7.45, 8.23]</code>).\n",
    "\n",
    "Para adaptar este enfoque para ajustar hiperparámetros de distinto tipo, vamos a representar cada hiperparámetro como un valor flotante. \n",
    "\n",
    "Posteriormente, definimos una forma de transformar cada parámetro a su representación original.\n",
    "\n",
    "Las transformaciones son las siguientes:\n",
    "\n",
    "* <code>n_estimators</code> : Redondeamos el valor del parámetro utilizando la función <code>int() </code>.\n",
    "* <code>learnig_rate</code> : No se requiere transformación.\n",
    "* <code>criterion</code> : el valor del parámetro utilizando la función <code>int() </code> y lo utilizamos para buscar el valor correspondiente en un diccionario que mapea enteros con las categorías.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519a798a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = wine.drop(columns = \"class\").to_numpy()\n",
    "y = wine[\"class\"].to_numpy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71854e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GradientBoostingClassifier(n_estimators = 20, learning_rate=0.4, \n",
    "                                 loss = \"log_loss\", criterion=\"friedman_mse\")\n",
    "\n",
    "cv_results = cross_val_score(clf, X_train, y_train, cv = 5, scoring = \"accuracy\")\n",
    "\n",
    "cv_results.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3777907",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Definimos la función a optimizar\n",
    "def loss_gbc(X):\n",
    "    \n",
    "    try:\n",
    "        criterion_transform = {0 : \"friedman_mse\", 1 : \"squared_error\"}\n",
    "\n",
    "        n_estimators_p = int(X[0])\n",
    "        learning_rate_p = X[1]\n",
    "        criterion_p = criterion_transform[int(X[2])]\n",
    "\n",
    "        clf = GradientBoostingClassifier(n_estimators = n_estimators_p, learning_rate=learning_rate_p, \n",
    "                                         loss = \"log_loss\", criterion=criterion_p, random_state=42)\n",
    "\n",
    "        cv_results = cross_val_score(clf, X_train, y_train, cv = 5, scoring = \"accuracy\")\n",
    "\n",
    "        return -cv_results.mean()\n",
    "    except:\n",
    "        return 10e10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8ab5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "############\n",
    "###   PSO\n",
    "############\n",
    "\"\"\"\n",
    "# Ejecutamos el algoritmo PSO\n",
    "# Tamaño de la población\n",
    "n = 10\n",
    "# Número de variables\n",
    "n_var = 3\n",
    "l_bounds = np.array([1, 0, 0])\n",
    "u_bounds = np.array([30, 20, 2])\n",
    "generaciones = 10\n",
    "# Social scaling parameter\n",
    "α = 0.5\n",
    "# Cognitive scaling parameter\n",
    "β = 0.8\n",
    "# velocity inertia\n",
    "w = 0.5\n",
    "\n",
    "x_best_pso, y_best_pso, fitness_pso  = PSO(loss_gbc, n, generaciones, n_var, u_bounds, l_bounds, α, β, w)\n",
    "x_best_pso, y_best_pso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90eb5814",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#####################\n",
    "### GENETICO BINARIO\n",
    "#####################\n",
    "\"\"\"\n",
    "\n",
    "# Parámetros especificos al Genético Binario\n",
    "\n",
    "precision = 6\n",
    "pro_cruza = 0.8\n",
    "\n",
    "x_best_genetico, y_best_genetico, fitness_genetico = genetico_binario(loss_gbc, n, generaciones,\n",
    "                                                     n_var, u_bounds, l_bounds, \n",
    "                                                     precision, pro_cruza)\n",
    "x_best_genetico, y_best_genetico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a0e358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparamos el fitness de los algoritmos\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "plt.plot(range(generaciones), fitness_pso, label =\"PSO\")\n",
    "plt.plot(range(generaciones), fitness_genetico, label =\"Genético Binario\")\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"Comparación de algoritmos de optimización\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6bdb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionamos el mejor vector\n",
    "min_value = 100000\n",
    "x_best = None\n",
    "algo_best = None\n",
    "\n",
    "for algo,x in zip([\"Genético\",\"PSO\"],[x_best_genetico,x_best_pso]):\n",
    "    if loss_gbc(x) < min_value:\n",
    "        min_value = loss_gbc(x)\n",
    "        x_best = x\n",
    "        algo_best = algo\n",
    "print(f\"El valor mínimo de la función es {min_value}\\nEl mejor vector fue el del algoritmo {algo_best}\\nx_best: {x_best}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5f341f",
   "metadata": {},
   "source": [
    "## EJERCICIO:  Heart Disease Data Set\n",
    "\n",
    "Encuentre el modelo con el mejor accuracy con el clasificador GBC para el conjunto de datos de Heart Disease ([liga](https://archive.ics.uci.edu/ml/datasets/heart+disease)).\n",
    "\n",
    "La columna <code>target</code> etiqueta como *1* la presencia de cardiopatía y 2 la ausencia de cardiopatía.\n",
    "\n",
    "Para mejorar el ajuste, puedes agregar uno o más hiperparámetros adicionales para ser ajustados. Visita la siguiente [liga](https://scikitlearn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html) para elegir el-los hiperparámetros que puedes agregar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39115fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Heart disease data set\n",
    "\n",
    "hear_disease = pd.read_csv(\"https://github.com/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/raw/master/Chapter06/heart_disease.csv\")\n",
    "hear_disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcac42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = hear_disease.drop(columns = \"target\").to_numpy()\n",
    "y = hear_disease[\"target\"].to_numpy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "clf = GradientBoostingClassifier(n_estimators = 20, learning_rate=0.4, \n",
    "                                 loss = \"log_loss\", criterion=\"friedman_mse\")\n",
    "\n",
    "cv_results = cross_val_score(clf, X_train, y_train, cv = 5, scoring = \"accuracy\")\n",
    "\n",
    "cv_results.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c32c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Definimos la función a optimizar\n",
    "def loss_gbc(X):\n",
    "    \n",
    "    try:\n",
    "        criterion_transform = {0 : \"friedman_mse\", 1 : \"squared_error\"}\n",
    "\n",
    "        n_estimators_p = int(X[0])\n",
    "        learning_rate_p = X[1]\n",
    "        criterion_p = criterion_transform[int(X[2])]\n",
    "\n",
    "        clf = GradientBoostingClassifier(n_estimators = n_estimators_p, learning_rate=learning_rate_p, \n",
    "                                         loss = \"log_loss\", criterion=criterion_p, random_state=42)\n",
    "\n",
    "        cv_results = cross_val_score(clf, X_train, y_train, cv = 5, scoring = \"accuracy\")\n",
    "\n",
    "        return -cv_results.mean()\n",
    "\n",
    "    except:\n",
    "        return 10e10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbda49e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ceaabf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff02dd17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

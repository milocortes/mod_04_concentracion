{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20f671ff-bfea-4e32-abe3-b6766fdbd4d9",
   "metadata": {},
   "source": [
    "# Ejercicios : Problemas de Optimización usando Métodos de primer orden\n",
    "<a href=\"https://colab.research.google.com/github/milocortes/mod_04_concentracion/blob/ccm-2024/src/talleres/metodos_primer_orden.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6b1ef1-5036-49a9-9fbb-094332de0eec",
   "metadata": {},
   "source": [
    "## Métodos de primer orden\n",
    "\n",
    "Todos los métodos de iteración requieren especificar un punto de inicio $\\boldsymbol{\\theta}_{0}$. En cada iteración $t$ realizan una actualización siguiendo la siguiente regla:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\t\\boldsymbol{\\theta}_{t+1} = \\boldsymbol{\\theta}_{t} + \\rho_t \\boldsymbol{d}_{t}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "donde $\\rho_t$ se le conoce como **tamaño de paso** o **tasa de aprendizaje**, y $\\boldsymbol{d}_t$ es una **dirección de descenso**. \n",
    "\n",
    "### Steepest Descent\n",
    "Cuando la dirección de descenso es igual al negativo del gradiente ($\\textit{i.e}$ $\\boldsymbol{d}_t = - \\boldsymbol{g}_t $)(Recuerda que el gradiente apunta en la dirección de máximo incremento en $f$, por eso el negativo apunta en la dirección de máxima disminución), la dirección se le conoce como de **steepest descent**.\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\t\\boldsymbol{\\theta}_{t+1} = \\boldsymbol{\\theta}_{t} - \\rho_t \\boldsymbol{g}_{t}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Utilizando una tasa de aprendizaje constante $\\rho_t = \\rho$, la regla de actualización es:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\t\\boldsymbol{\\theta}_{t+1} = \\boldsymbol{\\theta}_{t} - \\rho \\boldsymbol{g}_{t}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "### Momentum\n",
    "\n",
    "Sí agregamos el término de *momentum* $\\boldsymbol{m_t}$ al método de steepest descent el cuál tiene por ecuación de actualización\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\boldsymbol{\\theta}_{t+1} = \\boldsymbol{\\theta}_{t} - \\rho \\boldsymbol{g}_{t}\n",
    "\\end{equation}\n",
    "$$\n",
    "El *momentum* es incorporado de la siguiente manera:\n",
    "$$\n",
    " \\begin{equation}\n",
    "    \\boldsymbol{m}_{t+1} = \\beta\\boldsymbol{m}_{t} - \\boldsymbol{g}_{t}\n",
    "\\end{equation}\n",
    "$$\t\n",
    "$$\n",
    " \\begin{equation}\n",
    "    \\boldsymbol{\\theta}_{t+1} = \\boldsymbol{\\theta}_{t} - \\rho \\boldsymbol{m}_{t+1}\n",
    "\\end{equation}\n",
    "$$\n",
    "Un valor típico de $\\beta$ es 0.9. Para $\\beta=0$, el método se reduce a steepest descent. Podemos pensar$\\beta$ como un factor de escala o e intensidad de inercia. Inicialmente, $\\boldsymbol{m}_{t}=0$.\n",
    "\n",
    "### Momentum de Nesterov\n",
    "\n",
    "El método de Nesterov  definido en el formato estándar de *momentum* es el siguiente:\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\boldsymbol{m}_{t+1} = \\beta\\boldsymbol{m}_{t} - \\rho\\nabla f(\\boldsymbol{\\theta}_{t} + \\beta \\boldsymbol{m}_{t})\n",
    "\\end{equation}\n",
    "$$\n",
    "$$\n",
    " \\begin{equation}\n",
    "    \\boldsymbol{\\theta}_{t+1} = \\boldsymbol{\\theta}_{t} + \\boldsymbol{m}_{t+1}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "### Adam\n",
    "\n",
    "Kigma y Ba (2014) propusieron el método *Adam* (Adaptative moment estimation). El método usa el cuadrado del gradiente así como un término de *momentum*. Las reglas de actualización son las siguientes:\n",
    "$$\n",
    "\\begin{gather*} \n",
    "    \\boldsymbol{m}_t = \\beta_1 \\boldsymbol{m}_{t-1} + \\beta_1 (1-\\beta_1)\\boldsymbol{g}_t \\\\\n",
    "    \\boldsymbol{s}_t = \\beta_2 \\boldsymbol{s}_{t-1} + \\beta_2 (1-\\beta_1)\\boldsymbol{g}^2_t \\\\\n",
    "    \\boldsymbol{\\tilde{m}}_t = \\dfrac{\\boldsymbol{m}_t }{1-\\beta^t_1}\\\\\n",
    "    \\boldsymbol{\\tilde{s}}_t = \\dfrac{\\boldsymbol{s}_t }{1-\\beta^t_2}\\\\\n",
    "    \\boldsymbol{\\theta}_{t+1} = \\boldsymbol{\\theta}_{t} - \\rho \\dfrac{1}{\\sqrt{\\boldsymbol{\\tilde{s}}_t} + \\epsilon} \\boldsymbol{\\tilde{m}}_t\n",
    "\\end{gather*}\n",
    "$$\n",
    "    \n",
    "Los valores estándar de los parámetros son $\\beta_1=0.9$, $\\beta_2=0.999$ y $\\epsilon=10^{-6}$. Si definimos $\\beta_1=0$ y quitamos la corrección del sesgo, obtenemos el método RMSprop (Hinton, et al., 2012) que no utiliza *momentum*. \n",
    "Se suele utilizar una tasa de aprendizaje constante igual a $\\rho=0.001$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b126a05-d28f-44cb-88d1-80b598ffbe77",
   "metadata": {},
   "source": [
    "## Definición de Clases\n",
    "\n",
    "![OOP:Herencia](herencia.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a04c3fb-d0fc-4055-8238-b2fdd563eeec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "364e9ac5-7dd5-4b9c-869c-d525249d9a25",
   "metadata": {},
   "source": [
    "**Si no terminaste de copiar los algoritmos, instálalos del repositorio**:\n",
    "\n",
    "* https://github.com/milocortes/algo_optim_mod04\n",
    "\n",
    "Ejecuta la siguiente instrucción\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d4a6b4-96c6-4cab-adc8-6ee2e741e843",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/milocortes/algo_optim_mod04.git@main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a862a13f-6a6f-4883-bd7e-2702a9401681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función a minimizar\n",
    "import numpy as np \n",
    "from algo_optim_mod04.algoritmos_primer_orden import Steepest, Momentum, MomentumNesterov, Adam\n",
    "\n",
    "def f(X : np.array):\n",
    "    x,y = X\n",
    "    return 6*x**2 + 40*y**2 - 12*x - 30*y + 3\n",
    "\n",
    "# Función del gradiente \n",
    "def gradiente(X : np.array):\n",
    "    x, y = X\n",
    "    dx = 12*x - 12\n",
    "    dy = 80*y - 30\n",
    "    return np.array([dx,dy])\n",
    "\n",
    "\n",
    "## Probamos la clase Steepest\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3932f957-47c3-40a3-8a16-776d36ef5da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Probamos la clase Momentum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d0b97d-4ff5-4547-a7f8-de859e2643d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Probamos la clase Momentum Nesterov\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2798962b-7508-47ce-8e4e-0e7e101a93a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Probamos la clase Adam\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e94077-5c6c-412a-91f6-ab3565f20555",
   "metadata": {},
   "source": [
    "## Resolución de problemas de optimización con algoritmos de primer orden\n",
    "\n",
    "### Libro : Introduction to Computational Economics Using Fortran \n",
    "#### Capítulo 3. Numerical Solution Methods\n",
    "![Introduction to Computational Economics Using Fortran](compu_fortran.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb94ea1-d63e-4a17-b897-ac4356c48f34",
   "metadata": {},
   "source": [
    "### Ejercicio 2.4\n",
    "\n",
    "Considera el siguiente problema de optimización intertemporal de los hogares: La función de utilidad de los hogares está dada por\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    U(c_1,c_2,c_3) = \\sum_{i=1}^3 \\beta^{i-1} u(c_i)\\qquad \\text{con} \\qquad u(c_i) = \\dfrac{c_i^{1-\\frac{1}{\\gamma}}}{1 - \\frac{1}{\\gamma}}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "donde $c_i$ define el consumo en el periodo $i$ de vida, $\\beta$ denota la tasa de descuento y $\\gamma$ es la elasticidad de sustitución intertemporal.\n",
    "\n",
    "Asume que los hogares reciben ingreso laboral $w$ en los primeros dos periodos y consumen todos sus ahorros en el tercer periodo, de manera que la restricción presupuestaria queda definida como:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\sum_{i=1}^{3} \\dfrac{c_i}{(1+r)^{i-1}} = \\sum_{i=1}^{2} \\dfrac{w}{(1+r)^{i-1}}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "donde $r$ es la tasa de interés.\n",
    "\n",
    "Resuelve los niveles de consumo óptimo usando alguno de los métodos de optimización vistos hasta ahora.\n",
    "\n",
    "Procede de la siguiente manera:\n",
    "\n",
    "* a) Sustituye la restricción presupuestaria en la función de utilidad de manera que esta dependa únicamente de $c_2$ y $c_3$.\n",
    "* b) Minimiza la función $-\\widetilde{U}(c_2,c_3)$ con el objetivo de obtener los valores óptimos de $c_2$ y $c_3$ ($c_2^*$ y $c_3^*$).\n",
    "* c) Deriva $c_1^*$ de la restricción presupuestaria.\n",
    "\n",
    "Usa los siguientes parámetros:\n",
    "* $\\gamma=0.5$\n",
    "* $\\beta=1$\n",
    "\n",
    "Realiza el siguiente análisis de sensibilidad variando el valor de $w$ y $r$ de acuerdo a la siguiente tabla:\n",
    "\n",
    "|   $w$ |   $r$ |   $c_1$ |   $c_2$ |   $c_3$ |\n",
    "|------:|------:|--------:|------:|------:|\n",
    "|     1 |   0   |    0.66 |  0.66 |  0.66 |\n",
    "|     2 |   0   |    1.33 |  1.33 |  1.33 |\n",
    "|     1 |   0.1 |    0.66 |  0.69 |  0.73 |\n",
    "\n",
    "\n",
    "Los valores de $c_1$, $c_2$ y $c_3$ son los obtenidos por Fehr y Kindermann (2018)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d3526d-0d16-4fca-8b4d-0f2ccfd303cc",
   "metadata": {},
   "source": [
    "#### Solución: \n",
    "* a) Podemos reducir la dimensión del problema de optimización al reformular la restricción presupuestaria de la siguiente manera:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    c_1 = w + \\dfrac{w - c_2}{1+r} - \\dfrac{c_3}{(1+r)^2}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "sustituyendo la nueva restricción en nuestra función objetivo, reducimos nuestro problema a un problema en dos dimensiones sin restricciones:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\widetilde{U}(c_2,c_3) = \\dfrac{1}{1-\\frac{1}{\\gamma}} \\Bigg[  w + \\dfrac{w - c_2}{1+r} - \\dfrac{c_3}{(1+r)^2} \\Bigg]^{1-\\frac{1}{\\gamma}} + \\beta \\dfrac{c_2^{1-\\frac{1}{\\gamma}}}{1-\\frac{1}{\\gamma}}+ \\beta^2 \\dfrac{c_3^{1-\\frac{1}{\\gamma}}}{1-\\frac{1}{\\gamma}}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "* b) Reescribimos el problema de maximización de la utilidad en la siguiente forma:\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\min_{c_2,c_3} -\\widetilde{U}(c_2,c_3)\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "* **NOTA 1**: calcula el gradiente de la función $\\widetilde{U}(c_2,c_3)$ usando alguna paquetería de diferenciación automática.\n",
    "  \n",
    "* **NOTA 2**: inicia la búsqueda en el punto $[c_2,c_3] = [0.2, 0.2]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7aba31-8f70-428a-b7e6-09363ac5b64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numdifftools \n",
    "import numdifftools as nd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446d6382-6ece-434f-92cc-e38efa5d8ccd",
   "metadata": {},
   "source": [
    "### Libro : Ensemble Methods for Machine Learning\n",
    "#### Capítulo : Sequential Ensembles: Gradient Boosting\n",
    "\n",
    "![Ensemble Methods for Machine Learning](ensambles.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ef57f6-8a14-41bf-9c73-08147d49a212",
   "metadata": {},
   "source": [
    "#### 5.1.2 Gradient Descent over Loss Functions for Training\n",
    "\n",
    "Considera un problema de clasificación simple en espacio de características de 2 dimensiones.\n",
    "\n",
    "Generamos datos sintéticos de un problema de clasificación para obtener un data set linealmente separable para el cual podemos entrenar un separador linear o función de clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cce147-2ecc-4074-a325-6060a3332e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X, y = make_blobs(\n",
    "    n_samples = 200, n_features = 2, \n",
    "    centers = [[-1.5,-1.5], [1.5,1.5]], random_state = 42\n",
    ")\n",
    "\n",
    "plt.scatter(\n",
    "    x=X.T[0],\n",
    "    y=X.T[1],\n",
    "    c=y,\n",
    "    cmap=plt.cm.coolwarm, \n",
    "    edgecolors='k'\n",
    ")\n",
    "\n",
    "plt.title(\"Problema simple de clasificación\")\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e0db6a-1e1e-4aa0-8d7d-59466c66e53a",
   "metadata": {},
   "source": [
    "El clasificador lineal que queremos entrenar, $h(\\mathbf{x})$,  toma la siguiente forma:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "h_\\mathbf{w}(\\mathbf{x}) = w_1 x_1 + w_2 x_2,\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "el cual toma datos de entrenamiento $\\mathbf{x} = [x_1, x_2]^T$. El clasificador lineal es parametrizado por pesos $\\mathbf{w} = [w_1, w_2]^T$, los cuales tenemos que aprender usando los datos de entrenamiento. Con el objetivo de entrenar al clasificador, necesitamos definir una función de pérdida.\n",
    "\n",
    "Para este problema, usamos una **pérdida cuadrática** (**squared loss**) para el clasificador $h_\\mathbf{w}(\\mathbf{x})$ sobre el conjunto de entrenamiento de $n$ muestras $\\mathbf{x}_i = [x_1^i, x_2^i]^T$, $i=1, ..., n$, con sus respectivas etiquetas $y_i$.\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "f_{loss}(w_1, w_2) = \\frac{1}{2} \\sum_{i=1}^n \\left( y_i - h_\\mathbf{w}(\\mathbf{x}_i) \\right)^2 = \\frac{1}{2} \\sum_{i=1}^n \\left( y_i - w_1 x_1^i - w_2 x_2^i \\right)^2 = \\frac{1}{2} (\\mathbf{y} - X\\mathbf{w})^T (\\mathbf{y} - X\\mathbf{w}).\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94e9ed3-59cb-4af2-8991-d3dfc2ccdeab",
   "metadata": {},
   "source": [
    "Podemos calcular el gradiente de esta función de pérdida con respecto a $w_1$ y $w_2$.\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "g(w_1, w_2) = \\left[ \\begin{array}{c} \n",
    "    \\frac{\\partial f_{loss}(w_1, w_2)}{\\partial w_1} \\\\ \n",
    "    \\frac{\\partial f_{loss}(w_1, w_2)}{\\partial w_2} \n",
    "    \\end{array} \\right] \n",
    "    = \\left[ \\begin{array}{c} \n",
    "    - \\sum_{i=1}^n \\left( y_i - w_1 x_1 - w_2 x_2 \\right) x_1\\\\ \n",
    "    - \\sum_{i=1}^n \\left( y_i - w_1 x_1 - w_2 x_2 \\right) x_2 \\end{array} \\right]\n",
    "    = -X^T (\\mathbf{y} - X\\mathbf{w})\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "En las dos ecuaciones anteriores, las expresiones del extremo derecho son las versiones vectorizadas de la función de pérdida, donde $X$ es la matriz de datos y $\\mathbf{y}$ es el vector de etiquetas. La versión vectorizada es más compacta, más fácil y más eficiente de implementar ya que evita el ciclo for para la suma.\n",
    "\n",
    "Definimos las funciones de pérdida y su gradiente en Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf82ab02-2d70-4021-99c7-4933919f7ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Definimos la función de pérdida\n",
    "\n",
    "## Definimos el gradiente de la función de pérdida\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3774afda-ad9f-4e59-93a5-9bad2680493b",
   "metadata": {},
   "source": [
    "Utilizamos algún método de primer orden para obtener los pesos que minimizan la función de perdida, $\\mathbf{w}^* = [w_1^*, w_2^*]^T$. Iniciamos la búsqueda en el punto $\\mathbf{w} = [0.0, -0.99]^T$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afd88e5-ab6a-4e12-91a6-0e0825c2c4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Probamos la clase Adam\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fa7515-944e-43ef-9213-fe49425ec292",
   "metadata": {},
   "source": [
    "Visualizamos la frontera de decisión de nuestro clasificador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e145d48-fd0e-4b76-9e85-3c49b237c0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm, edgecolors='k')\n",
    "\n",
    "ax = plt.gca()\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "xx1, xx2 = np.meshgrid(np.linspace(xlim[0], xlim[1], 50),\n",
    "                   np.linspace(ylim[0], ylim[1], 50))\n",
    "\n",
    "Z = w[0]*xx1.ravel() + w[1]*xx2.ravel()\n",
    "Z = Z.reshape(xx1.shape)\n",
    "ax.contour(xx1, xx2, Z, colors='k', levels=[0], alpha=0.5,\n",
    "              linestyles=['--'])\n",
    "\n",
    "plt.title(\"Problema simple de clasificación\")\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51947872-3b5f-4623-b88c-b89eb69117d7",
   "metadata": {},
   "source": [
    "Dado que nuestro clasificador lineal $h_\\mathbf{w}(\\mathbf{x}) = w_1 x_1 + w_2 x_2,$ regresa valores de predicción reales, necesitamos convertirlos a 0 y 1. Para ello, asignamos la etiqueta $y_{pred} = 1$ a todas las predicciones positivas, mientras que para las predicciones negativas asignamos la etiqueta $y_{pred} = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398b3ceb-d6d4-4bb1-869b-fe4bce5563cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = (np.dot(X, w) >=0).astype(int)\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y, ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19164d72-a464-402b-9628-5f7ca89c4e84",
   "metadata": {},
   "source": [
    "### Ejercicio : Entrenamiento de un clasificador pseudo-lineal\n",
    "\n",
    "Considera un problema de clasificación simple en espacio de características de 2 dimensiones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1726fca3-fb06-4d81-8086-3bc78d9b6fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generamos datos sintéticos\n",
    "X, y = make_blobs(\n",
    "    n_samples = 400, n_features = 4, \n",
    "    centers = [[-2.5,-2.5], [2.5,2.5], [-2.5,2.5], [2.5,-2.5]], random_state = 42\n",
    ")\n",
    "\n",
    "## Ajustamos etiquetas\n",
    "y[y==3]=2\n",
    "y[y==0]=1\n",
    "\n",
    "y = y -1\n",
    "\n",
    "## Graficamos \n",
    "plt.scatter(\n",
    "    x=X.T[0],\n",
    "    y=X.T[1],\n",
    "    c=y,\n",
    "    cmap=plt.cm.coolwarm, \n",
    "    edgecolors='k'\n",
    ")\n",
    "\n",
    "plt.title(\"Problema simple de clasificación\")\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ae16a1-b834-4803-a807-12c87b324949",
   "metadata": {},
   "source": [
    "¿Es adecuado usar el clasificador lineal que usamos en el problema anterior, $h(\\mathbf{x})$?\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "h_\\mathbf{w}(\\mathbf{x}) = w_1 x_1 + w_2 x_2,\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Visita el siguiente sitio:\n",
    "* https://playground.tensorflow.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c25d740-18b2-4a7d-8b6c-d7ce8549fdd7",
   "metadata": {},
   "source": [
    "Entrena un clasificador lineal, $h(\\mathbf{x})$,  toma la siguiente forma:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "h_\\mathbf{w}(\\mathbf{x}) = w_1 x_1 + w_2 x_2 + w_3x_1x_2\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff36bea-d5f5-48ac-a224-9c8a57f097b5",
   "metadata": {},
   "source": [
    "La función de **pérdida cuadrática** (**squared loss**) para el clasificador $h_\\mathbf{w}(\\mathbf{x})$ sobre el conjunto de entrenamiento de $n$ muestras $\\mathbf{x}_i = [x_1^i, x_2^i]^T$, $i=1, ..., n$, con sus respectivas etiquetas $y_i$.\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "f_{loss}(w_1, w_2) = \\frac{1}{2} \\sum_{i=1}^n \\left( y_i - h_\\mathbf{w}(\\mathbf{x}_i) \\right)^2 = \\frac{1}{2} \\sum_{i=1}^n \\left( y_i - w_1 x_1^i - w_2 x_2^i - w_3 x_1^i x_2^i \\right)^2 .\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Podemos calcular el gradiente de esta función de pérdida con respecto a $w_1$, $w_2$ y $w_3$.\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "g(w_1, w_2, w_3) = \\left[ \\begin{array}{c} \n",
    "    \\frac{\\partial f_{loss}(w_1, w_2, w_3)}{\\partial w_1} \\\\ \n",
    "    \\frac{\\partial f_{loss}(w_1, w_2, w_3)}{\\partial w_2} \\\\\n",
    "    \\frac{\\partial f_{loss}(w_1, w_2, w_3)}{\\partial w_3} \n",
    "    \\end{array} \\right] \n",
    "    = \\left[ \\begin{array}{c} \n",
    "    - \\sum_{i=1}^n \\left( y_i - w_1 x_1 - w_2 x_2 - w_3 x_1 x_2 \\right) x_1\\\\ \n",
    "    - \\sum_{i=1}^n \\left( y_i - w_1 x_1 - w_2 x_2 - w_3 x_1 x_2\\right) x_2 \\\\\n",
    "    - \\sum_{i=1}^n \\left( y_i - w_1 x_1 - w_2 x_2 - w_3 x_1 x_2\\right) x_1x_2 \\end{array} \\right]\n",
    "\\end{equation}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbeebc7e-3cc4-46b5-b3d8-bfbbd976dc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Definimos la función de pérdida\n",
    "\n",
    "## Definimos el gradiente de la función de pérdida\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4baf8a7a-06fe-486c-a4ed-1dae82e5c72c",
   "metadata": {},
   "source": [
    "Utilizamos algún método de primer orden para obtener los pesos que minimizan la función de perdida, $\\mathbf{w}^* = [w_1^*, w_2^*, w_3^*]^T$. Iniciamos la búsqueda en el punto $\\mathbf{w} = [0.0, -0.99, -0.99]^T$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a2a1b7-7161-4fd8-958c-2734503c88ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Probamos la clase Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08317eb5-79af-40ae-aa09-37360105d68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm, edgecolors='k')\n",
    "\n",
    "ax = plt.gca()\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "xx1, xx2 = np.meshgrid(np.linspace(xlim[0], xlim[1], 50),\n",
    "                   np.linspace(ylim[0], ylim[1], 50))\n",
    "\n",
    "Z = w[0]*xx1.ravel() + w[1]*xx2.ravel() + w[2]*xx1.ravel()*xx2.ravel()\n",
    "Z = Z.reshape(xx1.shape)\n",
    "ax.contour(xx1, xx2, Z, colors='k', levels=[0], alpha=0.5,\n",
    "              linestyles=['--'])\n",
    "\n",
    "plt.title(\"Problema simple de clasificación\")\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d3c002-c55b-40dd-89c3-838423d502b6",
   "metadata": {},
   "source": [
    "Dado que nuestro clasificador lineal $h_\\mathbf{w}(\\mathbf{x}) = w_1 x_1 + w_2 x_2+ w_3x_1x_2,$ regresa valores de predicción reales, necesitamos convertirlos a 0 y 1. Para ello, asignamos la etiqueta $y_{pred} = 1$ a todas las predicciones positivas, mientras que para las predicciones negativas asignamos la etiqueta $y_{pred} = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fdda36-8a7f-481f-aa63-303858545f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = (w[0]*X[:,0] + w[1]*X[:,1] + w[2]*X[:,0]*X[:,1] >=0).astype(int)\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c018fe55-bc4c-4eb4-b658-db6bd3041a17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

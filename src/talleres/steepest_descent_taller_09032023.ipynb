{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Método Steepest Descent\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/milocortes/mod_04_concentracion/blob/ccm-2023/src/talles/steepest_descent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "Todos los métodos de iteración requieren especificar un punto de inicio $\\boldsymbol{\\theta}_{0}$. En cada iteración $t$ realizan una actualización siguiendo la siguiente regla:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\t\\boldsymbol{\\theta}_{t+1} = \\boldsymbol{\\theta}_{t} + \\rho_t \\boldsymbol{d}_{t}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "donde $\\rho_t$ se le conoce como **tamaño de paso** o **tasa de aprendizaje**, y $\\boldsymbol{d}_t$ es una **dirección de descenso**. \n",
    "\n",
    "Cuando la dirección de descenso es igual al negativo del gradiente ($\\textit{i.e}$ $\\boldsymbol{d}_t = - \\boldsymbol{g}_t $)(Recuerda que el gradiente apunta en la dirección de máximo incremento en $f$, por eso el negativo apunta en la dirección de máxima disminución), la dirección se le conoce como de **steepest descent**.\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\t\\boldsymbol{\\theta}_{t+1} = \\boldsymbol{\\theta}_{t} - \\rho_t \\boldsymbol{g}_{t}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Utilizando una tasa de aprendizaje constante $\\rho_t = \\rho$, la regla de actualización es:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\t\\boldsymbol{\\theta}_{t+1} = \\boldsymbol{\\theta}_{t} - \\rho \\boldsymbol{g}_{t}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Para el caso univariado, la regla de actualización es:\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\tx_{t+1} = x_{t} - \\rho f^\\prime (x_{t}) \n",
    "\\end{equation}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steepest Descent en 1D\n",
    "\n",
    "Sea la función univariada:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    f(x) = 6x^2 - 12x +3\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Graficamos la función"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como puese verse, la función es una parábola, por lo cual tiene un mínimo local (global). \n",
    "\n",
    "Obtengamos el mínimo de forma analítica al obtener la derivada e igualar a cero.\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\frac{d}{dx} (6x^2 - 12x +3) = 12x - 12 \n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Igualando $ 12x - 12=0$, tenemos que el mínimo es $x=1$.\n",
    "\n",
    "Obtengamos el minimo mediante el método steepest descent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steepest Descent en 2D\n",
    "\n",
    "Sea la función que recibe dos argumentos:\n",
    "\n",
    "$$\n",
    "    \\begin{equation}\n",
    "        f(x,y) = 6x^2 + 9y^2 - 12x -14y +3\n",
    "    \\end{equation}\n",
    "$$\n",
    "\n",
    "Obtenemos el gradiente:\n",
    "\n",
    "\n",
    "$$\n",
    "\\nabla f(x,y)=  \\begin{bmatrix}\n",
    "\\frac{\\partial f(x,y)}{\\partial x} \\\\\n",
    "\\frac{\\partial f(x,y)}{\\partial y}\n",
    "\\end{bmatrix} =\n",
    " \\begin{bmatrix}\n",
    "12x -12 \\\\\n",
    "18y -14\n",
    "\\end{bmatrix} \n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cambiemos la tasa de aprendizaje y la forma de la función\n",
    "\n",
    "La nueva función es:\n",
    "\n",
    "$$\n",
    "    \\begin{equation}\n",
    "        f(x,y) = 6x^2 + 40y^2 - 12x -30y +3\n",
    "    \\end{equation}\n",
    "$$\n",
    "\n",
    "Obtenemos el gradiente:\n",
    "\n",
    "\n",
    "$$\n",
    "\\nabla f(x,y)=  \\begin{bmatrix}\n",
    "\\frac{\\partial f(x,y)}{\\partial x} \\\\\n",
    "\\frac{\\partial f(x,y)}{\\partial y}\n",
    "\\end{bmatrix} =\n",
    " \\begin{bmatrix}\n",
    "12x -12 \\\\\n",
    "80y -30\n",
    "\\end{bmatrix} \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Método de Newton\n",
    "\n",
    "El método de Newton usa el gradiente e incorporan información sobre la curvatura de la función mediante la matriz Hessiana, ayudando a tener una convergencia más rápida.\n",
    "\n",
    "En cada iteración $t$ realizan una actualización siguiendo la siguiente regla:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\t\\boldsymbol{\\theta}_{t+1} = \\boldsymbol{\\theta}_{t} -   \\rho \\mathbf{H}_t^{-1}\\mathbf{g}_{t}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "donde $\\mathbf{H}_t^{-1}$ es la matriz Hessiana evaluada en $\\boldsymbol{\\theta}_{t} $.\n",
    "\n",
    "Usemos la misma función que en el ejemplo anterior:\n",
    "\n",
    "$$\n",
    "    \\begin{equation}\n",
    "        f(x,y) = 6x^2 + 40y^2 - 12x -30y +3\n",
    "    \\end{equation}\n",
    "$$\n",
    "\n",
    "Obtenemos el gradiente:\n",
    "\n",
    "\n",
    "$$\n",
    "\\nabla f(x,y)=  \\begin{bmatrix}\n",
    "\\frac{\\partial f(x,y)}{\\partial x} \\\\\n",
    "\\frac{\\partial f(x,y)}{\\partial y}\n",
    "\\end{bmatrix} =\n",
    " \\begin{bmatrix}\n",
    "12x -12 \\\\\n",
    "80y -30\n",
    "\\end{bmatrix} \n",
    "$$\n",
    "\n",
    "La matrix Hessiana es:\n",
    "\n",
    "$$\n",
    "\\nabla^2 f(x,y) = H=  \\begin{bmatrix}\n",
    "\\frac{\\partial f(x,y)}{\\partial x\\partial x} & \\frac{\\partial f(x,y)}{\\partial x\\partial y} \\\\\n",
    "\\frac{\\partial f(x,y)}{\\partial y\\partial x} & \\frac{\\partial f(x,y)}{\\partial y\\partial y}\n",
    "\\end{bmatrix} =\n",
    " \\begin{bmatrix}\n",
    "12 & 0 \\\\\n",
    "0 & 80\n",
    "\\end{bmatrix} \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparemos la convergencia de los tres métodos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
